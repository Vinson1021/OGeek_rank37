{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##在baseline_1基础上进行全局转化率平滑系数计算##\n",
    "##在baseline_2基础上添加nunique特征\n",
    "#在baseline_4基础上加feature_2和feature_3\n",
    "#在baseline_5基础上加feature的四则运算\n",
    "#!mkdir /home/admin/jupyter/env\n",
    "#!pip install --prefix=/home/admin/jupyter/env lightgbm\n",
    "import sys\n",
    "sys.path.append('env/lib/python3.6/site-packages/')\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from scipy import sparse\n",
    "start = time.time()\n",
    "import jieba\n",
    "\n",
    "##读取数据\n",
    "\n",
    "train = pd.read_table('Demo/DataSets/oppo_data_ronud2_20181107/data_train.txt', \n",
    "        names= ['prefix','query_prediction','title','tag','label'],quoting=3,\n",
    "                    header= None, encoding='utf-8').astype(str)\n",
    "test = pd.read_table('Demo/DataSets/oppo_round2_test_B/oppo_round2_test_B.txt', \n",
    "        names= ['prefix','query_prediction','title','tag','label'], quoting=3,header= None, encoding='utf-8').astype(str)\n",
    "valid = pd.read_table('Demo/DataSets/oppo_data_ronud2_20181107/data_vali.txt', \n",
    "        names= ['prefix','query_prediction','title','tag','label'],quoting=3, header= None, encoding='utf-8').astype(str)\n",
    "\n",
    "\n",
    "##去除异常值\n",
    "train = train[(train['label']=='0')|(train['label']=='1')].reset_index()\n",
    "del train['index']\n",
    "\n",
    "##打印数据形状、缺失情况、清理异常值\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)\n",
    "test['label'] = '-1'\n",
    "train.replace('nan',np.nan,inplace=True)\n",
    "valid.replace('nan',np.nan,inplace=True)\n",
    "test.replace('nan',np.nan,inplace=True)\n",
    "print(train.isnull().sum())\n",
    "print(valid.isnull().sum())\n",
    "print(test.isnull().sum())\n",
    "train['query_prediction'].fillna('{}',inplace=True)\n",
    "test['query_prediction'].fillna('{}',inplace=True)\n",
    "valid['query_prediction'].fillna('{}',inplace=True)\n",
    "train['title'].fillna('-1',inplace=True)\n",
    "def getSimilar(st1,st2):\n",
    "    num = 0\n",
    "    for s in st2:\n",
    "        if s in st1:\n",
    "            num+=1\n",
    "    return num/len(st1)\n",
    "# query 描述\n",
    "def getStatList2(s):\n",
    "    dct = json.loads(s)\n",
    "    se = pd.Series(dct).astype(float)\n",
    "    lst = se.describe().values.tolist()\n",
    "    lst.append(sum(se))\n",
    "    return lst\n",
    "def getFeature2(data): \n",
    "    df = pd.DataFrame()\n",
    "    df['query_prediction'] = data['query_prediction'].drop_duplicates().values\n",
    "    arr = df['query_prediction'].apply(getStatList2)\n",
    "    cols = []\n",
    "    for co in pd.Series(range(100)).describe().index.tolist():\n",
    "        cols.append('query_describe_'+co)\n",
    "    cols.append('query_sum')\n",
    "    df = pd.concat([df,pd.DataFrame(arr.values.tolist(),columns=cols)],axis=1)\n",
    "    return pd.merge(data,df,how='left',on='query_prediction')[cols]\n",
    "## prefix、title与 query交互\n",
    "def getStatList3(s,st):\n",
    "    dct = json.loads(s)\n",
    "    l = []\n",
    "    for k in dct.keys():\n",
    "        l.append(float(dct[k])*getSimilar(k,st))\n",
    "    lst = pd.Series(l).describe().values.tolist()\n",
    "    lst.append(sum(l))\n",
    "    if st in dct:\n",
    "        lst.append(float(dct[st]))\n",
    "    else:\n",
    "        lst.append(np.nan)\n",
    "    return lst\n",
    "def getFeature3(data): \n",
    "    df = data[['query_prediction','title']].drop_duplicates().reset_index()\n",
    "    del df['index']\n",
    "    arr1 = df.apply(lambda row:getStatList3(row['query_prediction'], row['title']),axis = 1)\n",
    "    cols1 = []\n",
    "    for co in pd.Series(range(100)).describe().index.tolist():\n",
    "        cols1.append('title_query_similar_'+co)\n",
    "    cols1.append('title_query_similar_sum')\n",
    "    cols1.append('title_query_prob')\n",
    "    df1 = pd.concat([df,pd.DataFrame(arr1.values.tolist(),columns=cols1)],axis=1)\n",
    "    \n",
    "    \n",
    "    df = data[['query_prediction','prefix']].drop_duplicates().reset_index()\n",
    "    del df['index']\n",
    "    arr2 = df.apply(lambda row:getStatList3(row['query_prediction'], row['prefix']),axis = 1)\n",
    "    cols2 = []\n",
    "    for co in pd.Series(range(100)).describe().index.tolist():\n",
    "        cols2.append('prefix_query_similar_'+co)\n",
    "    cols2.append('prefix_query_similar_sum')\n",
    "    cols2.append('prefix_query_prob')\n",
    "    df2 = pd.concat([df,pd.DataFrame(arr2.values.tolist(),columns=cols2)],axis=1)\n",
    "    \n",
    "    data = pd.merge(data,df1,how='left',on=['query_prediction','title'])\n",
    "    data = pd.merge(data,df2,how='left',on=['query_prediction','prefix'])\n",
    "    return data[cols1+cols2]\n",
    "try:\n",
    "    valid_feature = pd.read_csv('valid_query_describe.csv')\n",
    "except:\n",
    "    valid_feature = getFeature2(valid)\n",
    "    valid_feature.to_csv('valid_query_describe.csv',index=False)\n",
    "try:\n",
    "    test_feature = pd.read_csv('test_query_describe.csv')\n",
    "except:\n",
    "    test_feature = getFeature2(test)\n",
    "    test_feature.to_csv('test_query_describe.csv',index=False)\n",
    "\n",
    "try:\n",
    "    train_feature = pd.read_csv('train_query_describe.csv')\n",
    "except:\n",
    "    train_feature = getFeature2(train)\n",
    "    train_feature.to_csv('train_query_describe.csv',index=False)\n",
    "##############\n",
    "\n",
    "try:\n",
    "    valid_feature = pd.read_csv('valid_query_cross.csv')\n",
    "except:\n",
    "    valid_feature = getFeature3(valid)\n",
    "    valid_feature.to_csv('valid_query_cross.csv',index=False)\n",
    "try:\n",
    "    test_feature = pd.read_csv('test_query_cross.csv')\n",
    "except:\n",
    "    test_feature = getFeature3(test)\n",
    "    test_feature.to_csv('test_query_cross.csv',index=False)\n",
    "\n",
    "try:\n",
    "    train_feature = pd.read_csv('train_query_cross.csv')\n",
    "except:\n",
    "    train_feature = getFeature3(train)\n",
    "    train_feature.to_csv('train_query_cross.csv',index=False)\n",
    "###############################################################################################################################\n",
    "##合并数据与相似度描述特征\n",
    "# 合并到总的数据集\n",
    "train['part'] = 1\n",
    "valid['part'] = 2\n",
    "test['part'] = 3\n",
    "\n",
    "data = pd.concat([train,valid,test],axis=0,ignore_index=True)\n",
    "data['label'] = data['label'].apply(lambda x: int(x))\n",
    "train_index = data[data['label']>=0].index.tolist()\n",
    "test_index =  data[data['label']==-1].index.tolist()\n",
    "train_y = data.loc[train_index]['label']\n",
    "\n",
    "def get_cut_feature(dt):\n",
    "    df = pd.DataFrame()\n",
    "    for item in ['prefix', 'title']:\n",
    "        print(item)\n",
    "        stat = pd.DataFrame()\n",
    "        stat[item] = dt[item].drop_duplicates().values\n",
    "        stat[item+'_cut'] = stat[item].apply(lambda x:' '.join(jieba.cut(x, cut_all=False)))\n",
    "        df[item+'_cut'] = pd.merge(dt,stat,how='left',on=item)[item+'_cut']\n",
    "    stat = pd.DataFrame()\n",
    "    item = 'query_prediction'\n",
    "    print(item)\n",
    "    stat[item] = dt[item].drop_duplicates().values\n",
    "    def getList(x):\n",
    "        dct = json.loads(x)\n",
    "        lst = []\n",
    "        for k in dct.keys():\n",
    "            lst.extend(jieba.cut(k,cut_all=False))\n",
    "        return ' '.join(lst)\n",
    "    stat['query_prediction_cut'] = stat['query_prediction'].apply(getList)\n",
    "    df[item+'_cut'] = pd.merge(dt,stat,how='left',on=item)[item+'_cut']\n",
    "    return df\n",
    "df = get_cut_feature(data)\n",
    "cntv=CountVectorizer()\n",
    "vector_feature = ['prefix_cut','query_prediction_cut','title_cut']\n",
    "\n",
    "\n",
    "train_sp = pd.DataFrame()\n",
    "test_sp = pd.DataFrame()\n",
    "for feature in vector_feature:\n",
    "    print(feature)\n",
    "    cntv.fit(df[feature])\n",
    "    train_sp = sparse.hstack((train_sp,cntv.transform(df.loc[train_index][feature]))).tocsr()\n",
    "    test_sp = sparse.hstack((test_sp,cntv.transform(df.loc[test_index][feature]))).tocsr()\n",
    "del df\n",
    "#####\n",
    "def do_len(x):\n",
    "    try:\n",
    "        return len(x)\n",
    "    except:\n",
    "        return len(str(x))\n",
    "data['prefix_len'] = data.prefix.apply(do_len)\n",
    "data['title_len'] = data.title.apply(do_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valid_feature = pd.read_csv('valid_query_describe.csv')\n",
    "test_feature = pd.read_csv('test_query_describe.csv')\n",
    "train_feature = pd.read_csv('train_query_describe.csv')\n",
    "data_feature = pd.concat([train_feature,valid_feature,test_feature],axis=0,ignore_index=True)\n",
    "data = pd.concat([data,data_feature],axis=1)  \n",
    "\n",
    "valid_feature = pd.read_csv('valid_query_cross.csv')\n",
    "test_feature = pd.read_csv('test_query_cross.csv')\n",
    "train_feature = pd.read_csv('train_query_cross.csv')\n",
    "data_feature = pd.concat([train_feature,valid_feature,test_feature],axis=0,ignore_index=True)\n",
    "data = pd.concat([data,data_feature],axis=1)  \n",
    "\n",
    "del train\n",
    "del test\n",
    "del valid\n",
    "del data_feature\n",
    "del train_feature\n",
    "del valid_feature\n",
    "del test_feature\n",
    "\n",
    "\n",
    "data['prefix_title_similar'] = data.apply(lambda row:getSimilar(row['prefix'], row['title']),axis = 1)\n",
    "data['title_prefix_similar'] = data.apply(lambda row:getSimilar(row['title'], row['prefix']),axis = 1)\n",
    "\n",
    "\n",
    "data['prefix_length'] = data['prefix'].apply(lambda x:len(x))\n",
    "data['title_length'] = data['title'].apply(lambda x:len(x))\n",
    "data['ratio_length'] = data['prefix_length']/data['title_length']\n",
    "\n",
    "\n",
    "\n",
    "type_feature = ['prefix','query_prediction', 'title', 'tag']\n",
    "\n",
    "### labelencoder\n",
    "print('labelencodering...')\n",
    "for feature in type_feature:\n",
    "    se = pd.Series(data[feature].value_counts().index)\n",
    "    se = pd.Series(se.index,index=se.values)\n",
    "    data[feature] = data[feature].map(se)\n",
    "\n",
    "### click\n",
    "print('click...')\n",
    "n = len(type_feature)\n",
    "data['cnt']=1\n",
    "for i in range(n):\n",
    "    col_name = \"cnt_click_of_\"+type_feature[i]\n",
    "    se = data[type_feature[i]].map(data[type_feature[i]].value_counts())\n",
    "    data[col_name] =se.values\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n-i-1):\n",
    "        col_name = \"cnt_click_of_\"+type_feature[i+j+1]+\"_and_\"+type_feature[i]\n",
    "        s = time.time()\n",
    "        se = data.groupby([type_feature[i],type_feature[i+j+1]])['cnt'].sum()\n",
    "        dt = data[[type_feature[i],type_feature[i+j+1]]]\n",
    "        se = (pd.merge(dt,se.reset_index(),how='left',\n",
    "                        on=[type_feature[i],type_feature[j+i+1]]).sort_index()['cnt'].fillna(value=0)).astype(int)\n",
    "        data[col_name] = se.values\n",
    "        data[col_name+'_info'] = data[\"cnt_click_of_\"+type_feature[i]]*data[\"cnt_click_of_\"+type_feature[i+j+1]]/data[col_name]\n",
    "### ratio\n",
    "print('ratio...')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i!=j:\n",
    "            col_name = \"ratio_click_of_\"+type_feature[j]+\"_in_\"+type_feature[i]\n",
    "            se = data.groupby([type_feature[i],type_feature[j]])['cnt'].sum()\n",
    "            dt = data[[type_feature[i],type_feature[j]]]\n",
    "            cnt = data[type_feature[i]].map(data[type_feature[i]].value_counts())\n",
    "            try:\n",
    "                data[col_name] = ((pd.merge(dt,se.reset_index(),how='left',on=[type_feature[i],type_feature[j]]).sort_index()['cnt'].fillna(value=0)/cnt)*100).astype(int).values\n",
    "            except:\n",
    "                pass            \n",
    "del data['cnt']\n",
    "\n",
    "print('nunique...')\n",
    "def get_nunique_feature(type_feature):\n",
    "    df = pd.DataFrame()\n",
    "    n = len(type_feature)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i!=j:\n",
    "                try:\n",
    "                    col_name = \"nunique_\"+type_feature[j]+\"_in_\"+type_feature[i]\n",
    "                    se = data.groupby([type_feature[i]])[type_feature[j]].value_counts()\n",
    "                    se = pd.Series(1,index=se.index).sum(level=type_feature[i])\n",
    "                    df[col_name] = (data[type_feature[i]].map(se)).values\n",
    "                except:\n",
    "                    pass\n",
    "    return df\n",
    "df = get_nunique_feature(type_feature)\n",
    "data = pd.concat([data,df],axis=1)\n",
    "\n",
    "### cvr\n",
    "print('cvr...')\n",
    "def get_cvr_smooth_feature_total(type_feature):\n",
    "    from smooth import HyperParam\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    train = data.loc[train_index]\n",
    "    train_y = train['label']\n",
    "    n = len(type_feature)\n",
    "    df_ = pd.DataFrame()\n",
    "    #######################\n",
    "    n_parts = 6\n",
    "    skf = StratifiedKFold(n_splits=n_parts-1,random_state=20,shuffle=True)\n",
    "    data['n_parts'] = -1\n",
    "    num = 1\n",
    "    for trainIndex,testIndex in skf.split(train,train_y):\n",
    "        data.loc[testIndex,'n_parts'] = num\n",
    "        num+=1\n",
    "    data.loc[data['label']==-1,'n_parts']=6\n",
    "    #####################\n",
    "    rate = [1,1,1,1,1,0.8]\n",
    "    ###################\n",
    "    for co in type_feature:\n",
    "        col_name = 'cvr_of_'+co\n",
    "        se =pd.Series()\n",
    "        hp1 = []\n",
    "        hp2 = []\n",
    "        for k in range(1,n_parts+1):\n",
    "            df = data[data['n_parts']==k][[co]]\n",
    "            temp = data[(data['n_parts']!=k)&(data['label']>=0)&(data['part']==1)][[co,'label']].groupby(co)['label'].agg({co + '_click': 'sum', co + '_count': 'count'})\n",
    "            # smooth\n",
    "            temp[co + '_count'] = temp[co + '_count']*rate[k-1]\n",
    "            temp[co + '_click'] = temp[co + '_click']*rate[k-1]\n",
    "            hp1.extend(temp[co + '_count'].values.tolist())\n",
    "            hp2.extend(temp[co + '_click'].values.tolist())\n",
    "        HP = HyperParam(1, 1)\n",
    "        HP.update_from_data_by_moment(np.array(hp1), np.array(hp2))\n",
    "        a = HP.alpha\n",
    "        b = HP.beta\n",
    "        \n",
    "        for k in range(1,n_parts+1):\n",
    "            df = data[data['n_parts']==k][[co]]\n",
    "            temp = data[(data['n_parts']!=k)&(data['label']>=0)&(data['part']==1)][[co,'label']].groupby(co)['label'].agg({co + '_click': 'sum', co + '_count': 'count'})\n",
    "            # smooth\n",
    "            temp[co + '_count'] = temp[co + '_count']*rate[k-1]\n",
    "            temp[co + '_click'] = temp[co + '_click']*rate[k-1]\n",
    "            temp[co + '_ctr_smooth'] = (temp[co + '_click'] + a) / (\n",
    "                    temp[co + '_count'] + a+b)           \n",
    "            se = se.append(pd.Series(df[co].map(temp[co + '_ctr_smooth']).values, index = df.index))\n",
    "        df_[col_name+'_smooth'] = pd.Series(data.index).map(se)\n",
    "        \n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(n-i-1):\n",
    "            col_name = 'cvr2_of_'+type_feature[i]+\"_and_\"+type_feature[i+j+1]\n",
    "            se = pd.Series()\n",
    "            hp1 = []\n",
    "            hp2 = []\n",
    "            for k in range(1,n_parts+1):               \n",
    "                temp = data[(data['n_parts']!=k)&(data['label']>=0)&(data['part']==1)].groupby([type_feature[i],type_feature[i+j+1]])['label'].agg({col_name + '_click': 'sum', col_name + '_count': 'count'})\n",
    "                temp[col_name + '_count'] = temp[col_name + '_count']*rate[k-1]\n",
    "                temp[col_name + '_click'] = temp[col_name + '_click']*rate[k-1]\n",
    "                hp1.extend(temp[col_name + '_count'].values.tolist())\n",
    "                hp2.extend(temp[col_name + '_click'].values.tolist())\n",
    "            # smooth\n",
    "            HP = HyperParam(1, 1)\n",
    "            HP.update_from_data_by_moment(np.array(hp1), np.array(hp2))\n",
    "            a = HP.alpha\n",
    "            b = HP.beta\n",
    "            for k in range(1,n_parts+1):               \n",
    "                temp = data[(data['n_parts']!=k)&(data['label']>=0)&(data['part']==1)].groupby([type_feature[i],type_feature[i+j+1]])['label'].agg({col_name + '_click': 'sum', col_name + '_count': 'count'})\n",
    "\n",
    "                temp[col_name + '_count'] = temp[col_name + '_count']*rate[k-1]\n",
    "                temp[col_name + '_click'] = temp[col_name + '_click']*rate[k-1]\n",
    "                temp[col_name + '_ctr_smooth'] = (temp[col_name + '_click'] + a) / (\n",
    "                        temp[col_name + '_count'] + a+b)\n",
    "                dt = data[data['n_parts']==k][[type_feature[i],type_feature[i+j+1]]]\n",
    "                dt.insert(0,'index',list(dt.index))\n",
    "                dt = pd.merge(dt,temp[col_name + '_ctr_smooth'].reset_index(),how='left',on=[type_feature[i],type_feature[i+j+1]])\n",
    "                se = se.append(pd.Series(dt[col_name + '_ctr_smooth'].values,index=list(dt['index'].values)))\n",
    "            df_[col_name+'_smooth'] = pd.Series(data.index).map(se).values\n",
    "    del data['n_parts']\n",
    "    return df_\n",
    "df = get_cvr_smooth_feature_total(type_feature)\n",
    "data = pd.concat([data,df],axis=1)\n",
    "del data['part']\n",
    "################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_x = data.loc[train_index]\n",
    "\n",
    "test_x = data.loc[test_index].reset_index()\n",
    "del test_x['index']\n",
    "\n",
    "\n",
    "train_y = train_x.pop('label')\n",
    "del test_x['label']\n",
    "\n",
    "\n",
    "train_x = sparse.hstack((train_x,train_sp)).tocsr()\n",
    "\n",
    "test_x = sparse.hstack((test_x,test_sp)).tocsr()\n",
    "\n",
    "def searchBestCut(ytrue,ypre):\n",
    "    lst = []\n",
    "    for i in range(10,81,1):\n",
    "        lst.append(f1_score(ytrue,(pd.Series(ypre)>=i*0.01).astype(int).values))\n",
    "    se = pd.Series(lst,index=range(10,81,1)).sort_values()\n",
    "    return se\n",
    "\n",
    "\n",
    "params_default_lgb = {\n",
    "        'num_leaves':80, \n",
    "        'learning_rate':0.1, \n",
    "    'boosting':'gbdt',\n",
    "    'min_child_samples':20,\n",
    "    \n",
    "    'bagging_fraction':1, \n",
    "    'bagging_freq':1,\n",
    "    'feature_fraction':1, \n",
    "     'max_depth':-1,\n",
    "    'reg_alpha':2,\n",
    "    'reg_lambda':5, \n",
    "    'metric':'binary_logloss',\n",
    "    'objective':'binary'\n",
    "}\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "NBR = 20000\n",
    "VBE = 100\n",
    "ESR = 200\n",
    "\n",
    "EVAL_RESULT = {}\n",
    "train_part = lgb.Dataset(train_x[:-50000],label=train_y[:-50000])\n",
    "evals = lgb.Dataset(train_x[-50000:],label=train_y[-50000:])\n",
    "bst = lgb.train(params_default_lgb,train_part, \n",
    "      num_boost_round=NBR, valid_sets=[train_part,evals], \n",
    "      valid_names=['train','evals'],  early_stopping_rounds=ESR,\n",
    "      evals_result=EVAL_RESULT, verbose_eval=VBE)\n",
    "lst = EVAL_RESULT['evals']['binary_logloss']\n",
    "best_iter = lst.index(min(lst))+1\n",
    "\n",
    "evals_ypre =  bst.predict(train_x[-50000:],num_iteration = best_iter)\n",
    "se = searchBestCut(train_y[-50000:],evals_ypre)       \n",
    "cut,best_score = (se.index[-1])*0.01,se.values[-1]\n",
    "\n",
    "train_part = lgb.Dataset(train_x,label=train_y)\n",
    "bst = lgb.train(params_default_lgb,train_part, \n",
    "      num_boost_round=best_iter)\n",
    "test_ypre =  bst.predict(test_x,num_iteration = best_iter)\n",
    "res = pd.Series(test_ypre)\n",
    "\n",
    "filename = 'result/1202.csv'\n",
    "(res>=cut).astype(int).to_csv(filename,index=False)\n",
    "\n",
    "print((res>=cut).astype(int).value_counts())\n",
    "print(filename)\n",
    "newZip = zipfile.ZipFile('submission/1202.zip', 'w')\n",
    "newZip.write(filename, compress_type=zipfile.ZIP_DEFLATED)\n",
    "newZip.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
